"use strict";(self.webpackChunkfirehose=self.webpackChunkfirehose||[]).push([[592],{3905:function(e,t,n){n.d(t,{Zo:function(){return c},kt:function(){return m}});var r=n(7294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function a(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,o=function(e,t){if(null==e)return{};var n,r,o={},i=Object.keys(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var u=r.createContext({}),l=function(e){var t=r.useContext(u),n=t;return e&&(n="function"==typeof e?e(t):a(a({},t),e)),n},c=function(e){var t=l(e.components);return r.createElement(u.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},d=r.forwardRef((function(e,t){var n=e.components,o=e.mdxType,i=e.originalType,u=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),d=l(n),m=o,f=d["".concat(u,".").concat(m)]||d[m]||p[m]||i;return n?r.createElement(f,a(a({ref:t},c),{},{components:n})):r.createElement(f,a({ref:t},c))}));function m(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=n.length,a=new Array(i);a[0]=d;var s={};for(var u in t)hasOwnProperty.call(t,u)&&(s[u]=t[u]);s.originalType=e,s.mdxType="string"==typeof e?e:o,a[1]=s;for(var l=2;l<i;l++)a[l]=n[l];return r.createElement.apply(null,a)}return r.createElement.apply(null,n)}d.displayName="MDXCreateElement"},2338:function(e,t,n){n.r(t),n.d(t,{assets:function(){return c},contentTitle:function(){return u},default:function(){return m},frontMatter:function(){return s},metadata:function(){return l},toc:function(){return p}});var r=n(7462),o=n(3366),i=(n(7294),n(3905)),a=["components"],s={},u="BigQuery",l={unversionedId:"sinks/bigquery-sink",id:"sinks/bigquery-sink",title:"BigQuery",description:"Bigquery Sink has several responsibilities, first creation of bigquery table and dataset when they are not exist, second update the bigquery table schema based on the latest schema defined in stencil or infer from incoming data, third translate incoming messages into bigquery records and insert them to bigquery tables.",source:"@site/docs/sinks/bigquery-sink.md",sourceDirName:"sinks",slug:"/sinks/bigquery-sink",permalink:"/firehose/sinks/bigquery-sink",draft:!1,editUrl:"https://github.com/odpf/firehose/edit/master/docs/docs/sinks/bigquery-sink.md",tags:[],version:"current",frontMatter:{},sidebar:"docsSidebar",previous:{title:"JDBC",permalink:"/firehose/sinks/jdbc-sink"},next:{title:"InfluxDB",permalink:"/firehose/sinks/influxdb-sink"}},c={},p=[{value:"Asynchronous consumer mode",id:"asynchronous-consumer-mode",level:2},{value:"At Least Once Guarantee",id:"at-least-once-guarantee",level:2},{value:"Configurations",id:"configurations",level:2},{value:"Bigquery table schema update",id:"bigquery-table-schema-update",level:2},{value:"Protobuf and BigQuery table type mapping",id:"protobuf-and-bigquery-table-type-mapping",level:2},{value:"Partitioning",id:"partitioning",level:2},{value:"Clustering",id:"clustering",level:2},{value:"Kafka Metadata",id:"kafka-metadata",level:2},{value:"Default columns for json data type",id:"default-columns-for-json-data-type",level:2},{value:"Error handling",id:"error-handling",level:2},{value:"Google Cloud Bigquery IAM Permission",id:"google-cloud-bigquery-iam-permission",level:2}],d={toc:p};function m(e){var t=e.components,n=(0,o.Z)(e,a);return(0,i.kt)("wrapper",(0,r.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"bigquery"},"BigQuery"),(0,i.kt)("p",null,"Bigquery Sink has several responsibilities, first creation of bigquery table and dataset when they are not exist, second update the bigquery table schema based on the latest schema defined in stencil or infer from incoming data, third translate incoming messages into bigquery records and insert them to bigquery tables.\nBigquery utilise Bigquery ",(0,i.kt)("a",{parentName:"p",href:"https://cloud.google.com/bigquery/streaming-data-into-bigquery"},"Streaming API")," to insert record into bigquery tables. For more info on the sink refer to ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/odpf/depot/blob/main/docs/sinks/bigquery.md"},"Depot Bigquery sink documentation")),(0,i.kt)("h2",{id:"asynchronous-consumer-mode"},"Asynchronous consumer mode"),(0,i.kt)("p",null,"Bigquery Streaming API limits size of payload sent for each insert operations. The limitation reduces the amount of message allowed to be inserted when the message size is big.\nThis will reduce the throughput of bigquery sink. To increase the throughput, firehose provide kafka consumer asynchronous mode.\nIn asynchronous mode sink operation is executed asynchronously, so multiple sink task can be scheduled and run concurrently.\nThroughput can be increased by increasing the number of sink pool."),(0,i.kt)("h2",{id:"at-least-once-guarantee"},"At Least Once Guarantee"),(0,i.kt)("p",null,"Because of asynchronous consumer mode and the possibility of retry on the insert operation. There is no guarantee of the message order that successfully sent to the sink.\nThat also happened with commit offset, the there is no order of the offset number of the processed messages.\nFirehose collect all the offset sort them and only commit the latest continuous offset.\nThis will ensure all the offset being committed after messages successfully processed even when some messages are being re processed by retry handler or when the insert operation took a long time."),(0,i.kt)("h2",{id:"configurations"},"Configurations"),(0,i.kt)("p",null,"For Bigquery sink in Firehose we need to set first ","(",(0,i.kt)("inlineCode",{parentName:"p"},"SINK_TYPE"),"=",(0,i.kt)("inlineCode",{parentName:"p"},"bigquery"),")",". There are some generic configs which are common across different sink types which need to be set example: kafka consumer configs, the generic ones are mentioned in ",(0,i.kt)("a",{parentName:"p",href:"/firehose/advance/generic"},"generic.md"),". Bigquery sink specific configs are mentioned in depot ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/odpf/depot/blob/main/docs/reference/configuration/bigquery-sink.md"},"Depot-configuration/bigquery-sink.md section")),(0,i.kt)("h2",{id:"bigquery-table-schema-update"},"Bigquery table schema update"),(0,i.kt)("p",null,"Refer to ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/odpf/depot/blob/main/docs/sinks/bigquery.md#bigquery-table-schema-update"},"Depot-bigquery.md#bigquery-table-schema-update section")),(0,i.kt)("h2",{id:"protobuf-and-bigquery-table-type-mapping"},"Protobuf and BigQuery table type mapping"),(0,i.kt)("p",null,"For type conversion between protobuf to bigquery type. Please refer to\n",(0,i.kt)("a",{parentName:"p",href:"https://github.com/odpf/depot/blob/main/docs/sinks/bigquery.md#protobuf---bigquery-table-type-mapping"},"Depot-bigquery.md#protobuf-bigquery-table-type-mapping section")),(0,i.kt)("h2",{id:"partitioning"},"Partitioning"),(0,i.kt)("p",null,"Bigquery Sink supports creation of table with partition configuration.\nFor more information refer to ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/odpf/depot/blob/main/docs/sinks/bigquery.md#partitioning"},"Depot-bigquery.md#partitioning section")),(0,i.kt)("h2",{id:"clustering"},"Clustering"),(0,i.kt)("p",null,"Bigquery Sink supports for creating and modifying clustered or unclustered table with clustering configuration.\nFor more information refer to ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/odpf/depot/blob/main/docs/sinks/bigquery.md#clustering"},"Depot-bigquery.md#clustering section")),(0,i.kt)("h2",{id:"kafka-metadata"},"Kafka Metadata"),(0,i.kt)("p",null,"For data quality checking purpose sometimes kafka metadata need to be added on the record. For more information refer to ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/odpf/depot/blob/main/docs/sinks/bigquery.md#metadata"},"Depot-bigquery.md#metadata sectionn")),(0,i.kt)("h2",{id:"default-columns-for-json-data-type"},"Default columns for json data type"),(0,i.kt)("p",null,"With dynamic schema for json we need to create table with some default columns, example like parition key needs to be set during creation of the table. Sample config ",(0,i.kt)("inlineCode",{parentName:"p"},"SINK_BIGQUERY_DEFAULT_COLUMNS =event_timestamp=timestamp"),". For more information refer to ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/odpf/depot/blob/main/docs/sinks/bigquery.md#default-columns-for-json-data-type"},"Depot-bigquery.md#default-columns-for-json-data-type section")),(0,i.kt)("h2",{id:"error-handling"},"Error handling"),(0,i.kt)("p",null,"The response can contain multiple errors which will be sent to the firehose from depot. Please refer to ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/odpf/depot/blob/main/docs/sinks/bigquery.md#errors-handling"},"Depot-bigquery.md#errors-handling section")),(0,i.kt)("h2",{id:"google-cloud-bigquery-iam-permission"},"Google Cloud Bigquery IAM Permission"),(0,i.kt)("p",null,"Several IAM permission is required for bigquery sink to run properly. For more detail refer to ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/odpf/depot/blob/main/docs/sinks/bigquery.md#google-cloud-bigquery-iam-permission"},"Depot-bigquery.md#google-cloud-bigquery-iam-permission section")))}m.isMDXComponent=!0}}]);