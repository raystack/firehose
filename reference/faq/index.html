<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-reference/faq">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v0.0.0-5124">
<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-XXX"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-XXX",{})</script><title data-rh="true">Frequently Asked Questions | Firehose</title><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://odpf.github.io//firehose/reference/faq"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Frequently Asked Questions | Firehose"><meta data-rh="true" name="description" content="This section is an attempt to gather some commonly asked queries about Firehose and related components and provide some"><meta data-rh="true" property="og:description" content="This section is an attempt to gather some commonly asked queries about Firehose and related components and provide some"><link data-rh="true" rel="icon" href="/firehose/assets/favicon.ico"><link data-rh="true" rel="canonical" href="https://odpf.github.io//firehose/reference/faq"><link data-rh="true" rel="alternate" href="https://odpf.github.io//firehose/reference/faq" hreflang="en"><link data-rh="true" rel="alternate" href="https://odpf.github.io//firehose/reference/faq" hreflang="x-default"><link rel="stylesheet" href="/firehose/assets/css/styles.620b4533.css">
<link rel="preload" href="/firehose/assets/js/runtime~main.daa3e954.js" as="script">
<link rel="preload" href="/firehose/assets/js/main.2586b491.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();null!==e?t(e):window.matchMedia("(prefers-color-scheme: dark)").matches?t("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,t("light"))}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_fXgn">Skip to main content</a></div><div class="announcementBar_mb4j" style="background-color:#222;color:#eee" role="banner"><div class="announcementBarPlaceholder_vyr4"></div><div class="announcementBarContent_xLdY">⭐️ If you like Firehose, give it a star on <a target="_blank" rel="noopener noreferrer" href="https://github.com/odpf/firehose">GitHub</a>! ⭐</div><button type="button" class="clean-btn close announcementBarClose_gvF7" aria-label="Close"><svg viewBox="0 0 15 15" width="14" height="14"><g stroke="currentColor" stroke-width="3.1"><path d="M.75.75l13.5 13.5M14.25.75L.75 14.25"></path></g></svg></button></div><nav class="navbar navbar--fixed-top navbarHideable_m1mJ"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/firehose/"><div class="navbar__logo"><img src="/firehose/firehose.png" alt="Firehose" class="themedImage_ToTc themedImage--light_HNdA"><img src="/firehose/firehose.png" alt="Firehose" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Firehose</b></a></div><div class="navbar__items navbar__items--right"><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/firehose/">Documentation</a><a class="navbar__item navbar__link" href="/firehose/support">Support</a><a href="https://bit.ly/2RzPbtn" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-slack-link"></a><a href="https://github.com/odpf/firehose" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link navbar-item-github"></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex="-1" class="sidebarLogo_isFc" href="/firehose/"><img src="/firehose/firehose.png" alt="Firehose" class="themedImage_ToTc themedImage--light_HNdA"><img src="/firehose/firehose.png" alt="Firehose" class="themedImage_ToTc themedImage--dark_i4oU"><b>Firehose</b></a><nav class="menu thin-scrollbar menu_SIkG menuWithAnnouncementBar_GW3s"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/firehose/">Introduction</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="true" href="/firehose/guides/create_firehose">Guides</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/firehose/guides/create_firehose">Creating Firehose</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/firehose/guides/json-based-filters">JSON-based Filters</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/firehose/guides/jexl-based-filters">JEXL-based Filters</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/firehose/guides/deployment">Deployment</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/firehose/guides/manage">Troubleshooting</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="true" href="/firehose/sinks/http-sink">Sinks</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/firehose/sinks/http-sink">HTTP</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/firehose/sinks/grpc-sink">GRPC</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/firehose/sinks/jdbc-sink">JDBC</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/firehose/sinks/bigquery-sink">BigQuery</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/firehose/sinks/influxdb-sink">InfluxDB</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/firehose/sinks/prometheus-sink">Prometheus</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/firehose/sinks/mongo-sink">MongoDB</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/firehose/sinks/redis-sink">Redis</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/firehose/sinks/elasticsearch-sink">Elasticsearch</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/firehose/sinks/blob-sink">Blob</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/firehose/concepts/overview">Concepts</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/firehose/advance/generic">Advance</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/firehose/reference/metrics">Reference</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/firehose/reference/metrics">Metrics</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/firehose/reference/core-faqs">FAQs</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/firehose/reference/faq">Frequently Asked Questions</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/firehose/reference/glossary">Glossary</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/firehose/contribute/contribution">Contribute</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/firehose/roadmap">Roadmap</a></li></ul></nav></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_GujU"><div class="docItemContainer_Adtb"><article><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_aoJ5"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Frequently Asked Questions</h1><p>This section is an attempt to gather some commonly asked queries about Firehose and related components and provide some
answers.</p><h1>Table of contents</h1><ul><li><a href="#frequently-asked-questions">Frequently Asked Questions</a><ul><li><a href="#firehose-sinks">Firehose Sinks</a><ul><li><a href="#blob-sink">Blob Sink</a><ul><li><a href="#what-file-formats-are-the-records-written-into-">What file formats are the records written into ?</a></li><li><a href="#does-the-sink-support-jsonavroparquetcsv-">Does the sink support json/Avro/Parquet/CSV ?</a></li><li><a href="#does-the-sink-support-file-compression-gzipsnappylz4-">Does the sink support file compression GZIP/Snappy/lz4 ?</a></li><li><a href="#does-the-sink-support-partitioning-">Does the sink support partitioning ?</a></li><li><a href="#how-to-configure-partitioning-">How to configure partitioning ?</a></li><li><a href="#does-the-sink-support-s3-or-hdfs-">Does the sink support S3 or hdfs ?</a></li><li><a href="#how-the-schema-being-generated--how-the-data-types-being-converted-to-parquet-types-">How the schema being generated ? How the data types being converted to Parquet types ?</a></li><li><a href="#how-is-the-folder-path-for-the-file-that-being-created-">How is the folder path for the file that being created ?</a></li><li><a href="#how-to-configure-object-storage-bucket-">How to configure object storage bucket ?</a></li><li><a href="#what-will-happen-when-file-upload-failed-">What will happen when file upload failed ?</a></li><li><a href="#what-will-happen-when-disk-is-full-">What will happen when disk is full ?</a></li><li><a href="#how-to-implement-new-object-storage-provider-for-example-s3-">How to implement new Object Storage provider, for example S3 ?</a></li><li><a href="#how-to-implement-new-file-format-">How to implement new file format ?</a></li><li><a href="#how-to-commit-being-handled-">How to commit being handled ?</a></li><li><a href="#how-much-disk-size-recommended-for-the-sink-">How much Disk size recommended for the sink ?</a></li></ul></li><li><a href="#bigquery-sink">BigQuery Sink</a><ul><li><a href="#what-is-the-method-that-is-being-used-to-insert-the-bq-rows-">What is the method that is being used to insert the bq rows ?</a></li><li><a href="#does-the-sink-support-partitioned-table-">Does the sink support partitioned table ?</a></li><li><a href="#how-to-configure-partitioning-">How to configure partitioning ?</a></li><li><a href="#how-the-schema-being-generated--how-the-data-types-being-converted-to-bq-">How the schema being generated ? How the data types being converted to BQ ?</a></li><li><a href="#does-the-sink-support-ingestion-time-integer-range-partitioning">Does the sink support ingestion time/ integer range partitioning?</a></li><li><a href="#how-to-configure-table-destination-">How to configure table destination ?</a></li><li><a href="#what-will-happen-when-on-insertion-of-a-record-the-timestamp-is-out-of-range-more-than-5-year-in-the-past-or-1-year-in-the-future-">What will happen when on insertion of a record the timestamp is out of range more than 5 year in the past or 1 year in the future ?</a></li><li><a href="#how-many-records-are-inserted-batched-each-time-">How many records are inserted/ batched each time ?</a></li><li><a href="#when-is-the-bigquery-table-schema-updated-">When is the BigQuery table schema updated ?</a></li><li><a href="#is-the-table-automatically-created-">Is the table automatically created ?</a></li><li><a href="#does-this-sink-support-bigquery-table-clustering-configuration-">Does this sink support BigQuery table clustering configuration ?</a></li><li><a href="#does-this-sink-support-bigquery-table-labeling-">Does this sink support BigQuery table labeling ?</a></li></ul></li><li><a href="#redis-sink">Redis Sink</a><ul><li><a href="#what-is-the-redis-version-supported-">What is the Redis version supported ?</a></li><li><a href="#what-data-types-are-supported-in-redis-sink">What Data types are supported in Redis sink?</a></li><li><a href="#what-all-deployments-types-of-redis-is-supported-">What all deployments types of Redis is supported ?</a></li><li><a href="#how-to-use-redis-cluster-for-redis-sink">How to use Redis cluster for Redis sink?</a></li><li><a href="#how-to-specify-a-template-for-the-keys-">How to specify a template for the keys ?</a></li><li><a href="#how-to-select-nested-fields">How to select nested fields?</a></li><li><a href="#what-is-the-behaviour-on-connection-failures">What is the behaviour on connection failures?</a></li><li><a href="#how-can-ttl-be-configured-for-the-redis-keys">How can TTL be configured for the Redis keys?</a></li><li><a href="#does-it-support-deleting-the-keys">Does it support deleting the keys?</a></li><li><a href="#what-are-some-of-the-use-cases-of-this-sink">What are some of the use cases of this sink?</a></li><li><a href="#what-happens-if-the-redis-goes-down">What happens if the Redis goes down?</a></li></ul></li><li><a href="#jdbc-sink">JDBC Sink</a><ul><li><a href="#what-are-some-of-the-use-cases-of-this-sink">What are some of the use cases of this sink?</a></li><li><a href="#what-monitoring-metrics-are-available-for-this-sink">What monitoring metrics are available for this sink?</a></li><li><a href="#do-we-need-to-create-tableschema-before-pushing-data-via-jdbc-sink-">Do we need to create table/schema before pushing data via JDBC sink ?</a></li><li><a href="#any-restriction-of-version-of-supported-database">Any restriction of version of supported database?</a></li><li><a href="#how-messages-get-mapped-to-the-queries">How messages get mapped to the queries?</a></li><li><a href="#how-data-types-are-handled">How data types are handled?</a></li><li><a href="#how-are-the-database-connections-are-formed">How are the database connections are formed?</a></li><li><a href="#how-does-jdbc-sink-handles-connection-pooling">How does JDBC sink handles connection pooling?</a></li><li><a href="#when-and-how-do-the-db-connections-gets-closed">When and how do the DB connections gets closed?</a></li><li><a href="#how-to-support-a-new-data-base-which-supports-jdbc-eg-mysql-">How to support a new data base which supports JDBC, e:g MySQL ?</a></li><li><a href="#any-transaction-locking-provision">Any transaction, locking provision?</a></li><li><a href="#are-there-any-chances-of-race-conditions">Are there any chances of race conditions?</a></li></ul></li><li><a href="#http-sink">HTTP Sink</a><ul><li><a href="#how-does-the-payload-look-like">How does the payload look like?</a></li><li><a href="#does-it-support-delete-calls">Does it support DELETE calls?</a></li><li><a href="#how-many-messages-are-pushed-in-one-call">How many messages are pushed in one call?</a></li><li><a href="#how-can-i-configure-the-number-of-connections">How can I configure the number of connections?</a></li><li><a href="#what-data-types-of-request-body-are-supported">What data types of request body are supported?</a></li><li><a href="#does-it-support-client-side-load-balancing">Does it support client side load balancing?</a></li><li><a href="#which-authentication-methods-are-supported">Which authentication methods are supported?</a></li><li><a href="#how-can-i-pass-a-particular-input-field-as-a-header-in-request">How can I pass a particular input field as a header in request?</a></li><li><a href="#how-can-i-pass-a-particular-input-field-as-a-query-param-in-request">How can I pass a particular input field as a query param in request?</a></li><li><a href="#what-happens-if-my-services-fails-">What happens if my services fails ?</a></li><li><a href="#how-are-http-connections-handled-long-lived">How are HTTP connections handled, long lived?</a></li><li><a href="#what-is-logrequest-config">What is logRequest config?</a></li><li><a href="#what-is-shouldretry-config">What is shouldRetry config?</a></li><li><a href="#what-happens-in-case-of-null-response">What happens in case of null response?</a></li><li><a href="#what-happens-in-case-of-null-status-code-in-a-non-null-response">What happens in case of null status code in a non-null response?</a></li><li><a href="#when-is-a-message-dropped">When is a message dropped?</a></li><li><a href="#what-is-the-difference-between-parameterised-vs-dynamic-url-">What is the difference between Parameterised vs Dynamic Url ?</a></li><li><a href="#for-parameterised-header-how-is-the-data-added">For parameterised header, how is the data added?</a></li></ul></li><li><a href="#general-queries">General Queries</a><ul><li><a href="#which-java-versions-firehose-work-with">Which Java versions firehose work with?</a></li><li><a href="#how-does-the-execution-work-is-the-entire-process-sync-or-async-">How does the execution work? Is the entire process sync or async ?</a></li><li><a href="#is-there-any-code-snippet-which-shows-how-i-can-produce-sample-message-in-supported-data-format">Is there any code snippet which shows how I can produce sample message in supported data format?</a></li><li><a href="#why-protobuf--can-it-support-other-formats-like-json-avro-thrift-">Why Protobuf ? Can it support other formats like JSON, AVRO, Thrift ?</a></li><li><a href="#does-firehose-have-support-for-ssl-on-kafka">Does firehose have support for SSL on Kafka?</a></li><li><a href="#how-does--firehose-create-the-kafka-consumer">How does  Firehose create the Kafka consumer?</a></li><li><a href="#can-i-change-the-consumer-group-name-or-reset-it--overall-how-do-i-handle-my-consumer-group-operations">Can I change the consumer group name or reset it ? Overall, how do I handle my consumer group operations?</a></li><li><a href="#what-are-the-advantages-of-firehose-over-kafka-connect-">What are the advantages of Firehose over Kafka connect ?</a></li><li><a href="#what-problems-does-firehose-solve">What problems does Firehose solve?</a></li><li><a href="#can-i-do-any-transformations-before-sending-data-to-sink-for-example-filtering-">Can I do any transformations before sending data to sink, for example filtering ?</a></li><li><a href="#how-to-optimise-parallelism-based-on-input-rate-of-kafka-messages-does-it-depend-on-sink-">How to optimise parallelism based on input rate of Kafka messages? Does it depend on sink ?</a></li><li><a href="#what-are-the-retry-mechanisms-in-firehose">What are the retry mechanisms in firehose?</a></li><li><a href="#which-kafka-client-configs-are-available-">Which Kafka client configs are available ?</a></li><li><a href="#what-all-data-formats-are-supported">What all data formats are supported?</a></li><li><a href="#can-we-select-particular-fields-from-the-input-message">Can we select particular fields from the input message?</a></li><li><a href="#how-do-i-configure-the-protobuf-schema-for-the-topic-firehose-needs-to-consume">How do I configure the Protobuf schema for the topic Firehose needs to consume?</a></li><li><a href="#what-is-stencil-in-context-of-firehose-">What is Stencil in context of firehose ?</a></li><li><a href="#will-i-have-any-data-loss-if-my-firehose-is-failed-">Will I have any data loss if my firehose is failed ?</a></li><li><a href="#how-does-firehose-handle-failed-messages">How does firehose handle failed messages?</a></li><li><a href="#what-all-metrics-are-produced-for-me-to-monitor-">What all metrics are produced for me to monitor ?</a></li><li><a href="#what-kind-of-delivery-guarantees-does-firehose-provide-is-it-different-from-what-kafka-is-tuned-for">What kind of delivery guarantees does Firehose provide? Is it different from what Kafka is tuned for?</a></li><li><a href="#what-happens-if-my-firehose-is-stopped-and-kafka-retention-is-for-few-days">What happens if my firehose is stopped and Kafka retention is for few days?</a></li></ul></li></ul></li></ul></li></ul><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="firehose-sinks">Firehose Sinks<a class="hash-link" href="#firehose-sinks" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="blob-sink">Blob Sink<a class="hash-link" href="#blob-sink" title="Direct link to heading">​</a></h3><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-file-formats-are-the-records-written-into-">What file formats are the records written into ?<a class="hash-link" href="#what-file-formats-are-the-records-written-into-" title="Direct link to heading">​</a></h4><p>Files are written as Parquet file format. For more details, please
look <a href="https://parquet.apache.org/documentation/latest/" target="_blank" rel="noopener noreferrer">here</a></p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="does-the-sink-support-jsonavroparquetcsv-">Does the sink support json/Avro/Parquet/CSV ?<a class="hash-link" href="#does-the-sink-support-jsonavroparquetcsv-" title="Direct link to heading">​</a></h4><p>Though the sink only supports writing data as parquet files as of the latest release, other file formats can be added as
well.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="does-the-sink-support-file-compression-gzipsnappylz4-">Does the sink support file compression GZIP/Snappy/lz4 ?<a class="hash-link" href="#does-the-sink-support-file-compression-gzipsnappylz4-" title="Direct link to heading">​</a></h4><p>Firehose Blob Sink uses GZIP codec for data compression during the generation of Parquet files.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="does-the-sink-support-partitioning-">Does the sink support partitioning ?<a class="hash-link" href="#does-the-sink-support-partitioning-" title="Direct link to heading">​</a></h4><p>Yes, Firehose Blob Sink supports hourly and date wise partitioning.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-to-configure-partitioning-">How to configure partitioning ?<a class="hash-link" href="#how-to-configure-partitioning-" title="Direct link to heading">​</a></h4><p>Partitioning can be configured by setting the config <strong>SINK_BLOB_FILE_PARTITION_TIME_GRANULARITY_TYPE</strong> to HOUR for
hourly partitions or to DATE for date based partitioning. The timestamp field of the message which needs to be used for
partitioning can be configured via <strong>SINK_BLOB_FILE_PARTITION_PROTO_TIMESTAMP_FIELD_NAME</strong> config.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="does-the-sink-support-s3-or-hdfs-">Does the sink support S3 or hdfs ?<a class="hash-link" href="#does-the-sink-support-s3-or-hdfs-" title="Direct link to heading">​</a></h4><p>Though the sink only supports uploading files to Google Cloud Storage(GCS) as of the latest release, other storage
providers can be added as well.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-the-schema-being-generated--how-the-data-types-being-converted-to-parquet-types-">How the schema being generated ? How the data types being converted to Parquet types ?<a class="hash-link" href="#how-the-schema-being-generated--how-the-data-types-being-converted-to-parquet-types-" title="Direct link to heading">​</a></h4><p>Schema of the Kafka message is read from the proto class specified via configs. Stencil is used to serialize the message
byte array into a Protobuf Dynamic Message using this proto. It is then finally written to a parquet file. In case a
stencil registry is configured, the schema changes will be dynamically updated.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-is-the-folder-path-for-the-file-that-being-created-">How is the folder path for the file that being created ?<a class="hash-link" href="#how-is-the-folder-path-for-the-file-that-being-created-" title="Direct link to heading">​</a></h4><p>The filename is generated as a random UUID string. For the local file, base directory path is taken from the config<br>
<strong>SINK_BLOB_LOCAL_DIRECTORY</strong> and by default it is set to <code>/tmp/firehose</code>. Within this directory, subdirectories<br>
<!-- -->are generated with names based on whether partitioning is enabled or not:</p><ol><li>If <strong>SINK_BLOB_FILE_PARTITION_TIME_GRANULARITY_TYPE</strong> is set to NONE meaning partitioning is disabled, the parquet
files created are placed in directories with name set to the topic name of the kafka messages being processed.</li><li>If <strong>SINK_BLOB_FILE_PARTITION_TIME_GRANULARITY_TYPE</strong> is set to DAY, the parquet files generated are placed in
directories<br>with names in the format <!-- -->[topic-name]<!-- -->/<!-- -->[date-prefix][date]<!-- -->, where,<br>i. <!-- -->[topic-name]<!-- --> is the topic name of the Kafka message<br>ii. <!-- -->[date-prefix]<!-- --> is a config fetched from <strong>SINK_BLOB_FILE_PARTITION_TIME_DATE_PREFIX</strong> and is set to <code>dt=</code> as
default,<br>iii. <!-- -->[date]<!-- --> is the date in <code>yyyy-MM-dd</code> format extracted from a timestamp column of the Kafka message as specified
via<br><strong>SINK_BLOB_FILE_PARTITION_PROTO_TIMESTAMP_FIELD_NAME</strong> and converted to the timezone of<br><strong>SINK_BLOB_FILE_PARTITION_PROTO_TIMESTAMP_TIMEZONE</strong> (default UTC)</li><li>If <strong>SINK_OBJECT_STORAGE_TIME_PARTITIONING_TYPE</strong> is set to HOUR, the parquet files generated are placed in
directories<br>with names in the format <!-- -->[topic-name]<!-- -->/<!-- -->[date-prefix][date]<!-- -->/<!-- -->[hour-prefix][hour]<!-- -->, where,<br>i. <!-- -->[hour-prefix]<!-- --> is a config fetched from <strong>SINK_BLOB_FILE_PARTITION_TIME_HOUR_PREFIX</strong> and is set to <code>hr=</code> as
default,<br>ii. <!-- -->[hour]<!-- --> is the hour in <code>HH</code> format extracted from a timestamp column of the Kafka message as specified via<br><strong>SINK_BLOB_FILE_PARTITION_PROTO_TIMESTAMP_FIELD_NAME</strong> and converted to the timezone of<br><strong>SINK_BLOB_FILE_PARTITION_PROTO_TIMESTAMP_TIMEZONE</strong>(default UTC).</li></ol><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-to-configure-object-storage-bucket-">How to configure object storage bucket ?<a class="hash-link" href="#how-to-configure-object-storage-bucket-" title="Direct link to heading">​</a></h4><p>Firehose will automatically configure the GCS bucket upon starting by reading the credentials, project id, bucket name
and other settings as configured via environment variables. Please look at the Blob Sink configs for more details.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-will-happen-when-file-upload-failed-">What will happen when file upload failed ?<a class="hash-link" href="#what-will-happen-when-file-upload-failed-" title="Direct link to heading">​</a></h4><p>In case the file upload fails, the thread processing the file upload throws an exception which ultimately bubbles up and
causes the Firehose JVM to get terminated with an abnormal status code.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-will-happen-when-disk-is-full-">What will happen when disk is full ?<a class="hash-link" href="#what-will-happen-when-disk-is-full-" title="Direct link to heading">​</a></h4><p>In case the file upload fails, an IOException is thrown which ultimately bubbles up and causes the Firehose JVM to get
terminated with an abnormal status code.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-to-implement-new-object-storage-provider-for-example-s3-">How to implement new Object Storage provider, for example S3 ?<a class="hash-link" href="#how-to-implement-new-object-storage-provider-for-example-s3-" title="Direct link to heading">​</a></h4><p>Firehose exposes an interface class called <em>BlobStorage.java</em> which can be used to create new blob storage
implementations. Subsequently, the <em>BlobStorageFactory.java</em> can be modified to produce instance of this new blob
storage provider, while injecting the necessary configs. Firehose also exposes a config called as <strong>
SINK_BLOB_STORAGE_TYPE</strong> which is used to control which storage provider to use, with its default currently set to GCS.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-to-implement-new-file-format-">How to implement new file format ?<a class="hash-link" href="#how-to-implement-new-file-format-" title="Direct link to heading">​</a></h4><p>Firehose exposes an interface called <em>LocalFileWriter.java</em> which can be used to create new file writer implementations.
Subsequently, the <em>LocalStorage.java</em> class can be modified to produce instance of this new file writer, while injecting
the necessary configs. Firehose also exposes a config called as <strong>SINK_BLOB_LOCAL_FILE_WRITER_TYPE</strong> which is used to
control which file writer type to use, with its default currently set to parquet.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-to-commit-being-handled-">How to commit being handled ?<a class="hash-link" href="#how-to-commit-being-handled-" title="Direct link to heading">​</a></h4><p>ObjectStorage sink does its own offset management. After the polled kafka messages are processed by the sink and the
parquet files are uploaded to object storage and cleaned from local disk as per the policy, committable offsets are then
recorded by the sink. The offsets are then committed as per the Offset Manager. Please look at the Offset Manager class
for more details.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-much-disk-size-recommended-for-the-sink-">How much Disk size recommended for the sink ?<a class="hash-link" href="#how-much-disk-size-recommended-for-the-sink-" title="Direct link to heading">​</a></h4><p>A safe disk size will depend on and can be tuned based on configs set such as the time duration since creation after
which files are pushed to the Blob Storage (<strong>SINK_BLOB_LOCAL_FILE_ROTATION_DURATION_MS</strong>), max parquet file size after
which it is pushed to the Blob Storage(<strong>SINK_BLOB_LOCAL_FILE_ROTATION_MAX_SIZE_BYTES</strong>) as well as the number of topics
that Firehose is consuming from.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="bigquery-sink">BigQuery Sink<a class="hash-link" href="#bigquery-sink" title="Direct link to heading">​</a></h3><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-is-the-method-that-is-being-used-to-insert-the-bq-rows-">What is the method that is being used to insert the bq rows ?<a class="hash-link" href="#what-is-the-method-that-is-being-used-to-insert-the-bq-rows-" title="Direct link to heading">​</a></h4><p>Rows are inserted using streaming insert.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="does-the-sink-support-partitioned-table-">Does the sink support partitioned table ?<a class="hash-link" href="#does-the-sink-support-partitioned-table-" title="Direct link to heading">​</a></h4><p>Yes, BigQuery sink supports writing to partitioned tables as well.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-to-configure-partitioning--1">How to configure partitioning ?<a class="hash-link" href="#how-to-configure-partitioning--1" title="Direct link to heading">​</a></h4><p>Partitioning can be enabled by setting the config <strong>SINK_BIGQUERY_ENABLE_TABLE_PARTITIONING_ENABLE</strong> to true. A
partitioning key can be specified via the config <strong>SINK_BIGQUERY_TABLE_PARTITION_KEY</strong>. The time for expiry for date in
a partition can also be configured using <strong>SINK_BIGQUERY_TABLE_PARTITION_EXPIRY_MS</strong>.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-the-schema-being-generated--how-the-data-types-being-converted-to-bq-">How the schema being generated ? How the data types being converted to BQ ?<a class="hash-link" href="#how-the-schema-being-generated--how-the-data-types-being-converted-to-bq-" title="Direct link to heading">​</a></h4><p>The schema of the BQ table is generated as per the schema of the input Kafka message proto and is specified via <strong>
INPUT_SCHEMA_PROTO_CLASS</strong>. The Protobuf types are mapped to Legacy SQL types ( BigQuery types). Nested fields and lists
are mapped as type RECORD and REPEATED fields respectively. If Stencil is configured to dynamically fetch the updated
proto, the schema of the BQ table is modified accordingly as and when the input message&#x27;s proto changes in the Stencil
cache. As per constraints set by BigQuery, a maximum of 15 levels of nesting can be de-serialised and mapped into
corresponding BQ data types beyond which the field values are simply serialised into strings. During the serialisation,
the top level field names in the input Kafka message are mapped to column names in the Big Query table.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="does-the-sink-support-ingestion-time-integer-range-partitioning">Does the sink support ingestion time/ integer range partitioning?<a class="hash-link" href="#does-the-sink-support-ingestion-time-integer-range-partitioning" title="Direct link to heading">​</a></h4><p>No. As of now, Firehose BQ Sink only supports TimePartitioning. The only supported partition keys can be of type DATE or
TIMESTAMP.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-to-configure-table-destination-">How to configure table destination ?<a class="hash-link" href="#how-to-configure-table-destination-" title="Direct link to heading">​</a></h4><p>The geographic location of the dataset can be set using <strong>SINK_BIGQUERY_DATASET_LOCATION</strong>. Please note that this is a
one-time only change and becomes immutable once set. Changing the config later will cause an exception to be thrown. The
name of the table can be configured by specifying the config <strong>SINK_BIGQUERY_TABLE_NAME</strong>.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-will-happen-when-on-insertion-of-a-record-the-timestamp-is-out-of-range-more-than-5-year-in-the-past-or-1-year-in-the-future-">What will happen when on insertion of a record the timestamp is out of range more than 5 year in the past or 1 year in the future ?<a class="hash-link" href="#what-will-happen-when-on-insertion-of-a-record-the-timestamp-is-out-of-range-more-than-5-year-in-the-past-or-1-year-in-the-future-" title="Direct link to heading">​</a></h4><p>Firehose uses the
legacy <a href="https://cloud.google.com/bigquery/docs/reference/rest/v2/tabledata/insertAll" target="_blank" rel="noopener noreferrer">BQ Streaming API</a> and only allows
setting a partition key of type DATE or TIMESTAMP column. When Firehose streams data to the table, BigQuery
automatically puts the data into the correct partition, based on the values in the partition column. Records with
partition column value that is between 5 years in the past and 1 year in the future can be inserted, however data
outside this range is rejected. If <strong>SINK_BIGQUERY_TABLE_PARTITION_EXPIRY_MS</strong> is set, then BigQuery will delete the
data in the partition once this much time has elapsed since the creation of the partition.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-many-records-are-inserted-batched-each-time-">How many records are inserted/ batched each time ?<a class="hash-link" href="#how-many-records-are-inserted-batched-each-time-" title="Direct link to heading">​</a></h4><p>All the records fetched in one poll from the Kafka broker are pushed in one batch to BigQuery. This can be controlled by
the config <strong>SOURCE_KAFKA_CONSUMER_CONFIG_MAX_POLL_RECORDS</strong>.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="when-is-the-bigquery-table-schema-updated-">When is the BigQuery table schema updated ?<a class="hash-link" href="#when-is-the-bigquery-table-schema-updated-" title="Direct link to heading">​</a></h4><p>If Stencil is configured to dynamically fetch the updated proto, the schema of the BigQuery table is modified
accordingly as and when the input message&#x27;s proto changes in the Stencil cache.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="is-the-table-automatically-created-">Is the table automatically created ?<a class="hash-link" href="#is-the-table-automatically-created-" title="Direct link to heading">​</a></h4><p>Yes, Firehose will follow an upsert based model whereby if the BQ table or the dataset doesn&#x27;t exist, then it will
create them based on the set configs prior to pushing the records.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="does-this-sink-support-bigquery-table-clustering-configuration-">Does this sink support BigQuery table clustering configuration ?<a class="hash-link" href="#does-this-sink-support-bigquery-table-clustering-configuration-" title="Direct link to heading">​</a></h4><p>No, table clustering is not yet supported as of the latest release.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="does-this-sink-support-bigquery-table-labeling-">Does this sink support BigQuery table labeling ?<a class="hash-link" href="#does-this-sink-support-bigquery-table-labeling-" title="Direct link to heading">​</a></h4><p>Yes, both table labels and dataset labels can be specified by setting them in configs <strong>SINK_BIGQUERY_TABLE_LABELS</strong>
and <strong>SINK_BIGQUERY_DATASET_LABELS</strong> respectively. For example, <strong>SINK_BIGQUERY_TABLE_LABELS</strong> can be set
to <code>label1=value1,label2=value2</code>.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="redis-sink">Redis Sink<a class="hash-link" href="#redis-sink" title="Direct link to heading">​</a></h3><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-is-the-redis-version-supported-">What is the Redis version supported ?<a class="hash-link" href="#what-is-the-redis-version-supported-" title="Direct link to heading">​</a></h4><p>Firehose uses Jedis v3.0.1 (as of the latest release) as the redis client. At the time of writing this documentation,
Jedis is <a href="https://github.com/redis/jedis#jedis" target="_blank" rel="noopener noreferrer">fully compatible</a> with redis 2.8.x, 3.x.x and above.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-data-types-are-supported-in-redis-sink">What Data types are supported in Redis sink?<a class="hash-link" href="#what-data-types-are-supported-in-redis-sink" title="Direct link to heading">​</a></h4><p>Redis Sink supports persisting the input Kafka messages as LIST and HASHSET data types into Redis.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-all-deployments-types-of-redis-is-supported-">What all deployments types of Redis is supported ?<a class="hash-link" href="#what-all-deployments-types-of-redis-is-supported-" title="Direct link to heading">​</a></h4><p>Redis Sink currently supports Standalone and Cluster deployments of Redis.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-to-use-redis-cluster-for-redis-sink">How to use Redis cluster for Redis sink?<a class="hash-link" href="#how-to-use-redis-cluster-for-redis-sink" title="Direct link to heading">​</a></h4><p>To use Redis cluster as the sink, set the following configs as follows:</p><ol><li><strong>SINK_REDIS_URLS</strong> to be set to the cluster node URLs separated by comma as a delimiter. For
example, <code>127.0.0.1:30004,127.0.0.1:30002,127.0.0.1:30003</code></li><li><strong>SINK_REDIS_DEPLOYMENT_TYPE</strong> to be set as CLUSTER</li></ol><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-to-specify-a-template-for-the-keys-">How to specify a template for the keys ?<a class="hash-link" href="#how-to-specify-a-template-for-the-keys-" title="Direct link to heading">​</a></h4><p>The template key can be specified using the config <strong>SINK_REDIS_KEY_TEMPLATE</strong>. This can be set to either :</p><ol><li>A constant</li><li>As a template string: for example, <code>Service\_%%s,1</code>. Here, Firehose will extract the value of the incoming message
field having protobuf index 1 and create the Redis key as per the template.</li></ol><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-to-select-nested-fields">How to select nested fields?<a class="hash-link" href="#how-to-select-nested-fields" title="Direct link to heading">​</a></h4><p>Nested fields can only be selected from the incoming Kafka message when the data type to be pushed to Redis is set as
HashSet. To do so, ensure <strong>SINK_REDIS_DATA_TYPE</strong> config is set to HASHSET and <strong>INPUT_SCHEMA_PROTO_TO_COLUMN_MAPPING</strong>
config is set as a JSON indicating the mapping of input message proto field numbers to the desired field names in the
redis hash set entry. For example, consider a proto as follows:-</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">message Driver {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    int32 driver_id = 1;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    PersonName driver_name = 2;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">message PersonName {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    string fname = 1;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    string lname = 2;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Now consider that each entry in the Redis is required to be a HASHSET consisting of driver id as the key and the
driver&#x27;s first name and last name as the value fields with names as <code>first_name</code> and <code>last_name</code> respectively.
Accordingly, the <strong>INPUT_SCHEMA_PROTO_TO_COLUMN_MAPPING</strong> config can be set as follows:-</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">{&quot;&quot;2&quot;&quot;:&quot;&quot;{\&quot;&quot;1\&quot;&quot;:\&quot;&quot;first_name\&quot;&quot;, \&quot;&quot;2\&quot;&quot;:\&quot;&quot;last_name\&quot;&quot;}&quot;&quot;}&quot;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-is-the-behaviour-on-connection-failures">What is the behaviour on connection failures?<a class="hash-link" href="#what-is-the-behaviour-on-connection-failures" title="Direct link to heading">​</a></h4><p>If messages failed to get pushed to Redis because of some error( either due to connection timeout or Redis server
failure), Firehose will retry sending the messages for a fixed number of attempts depending on whether a retryable
exception was thrown. If the messages still failed to get published after retries, Firehose will push these messages to
the DLQ topic if DLQ is enabled via configs. If not enabled, it will drop the messages.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-can-ttl-be-configured-for-the-redis-keys">How can TTL be configured for the Redis keys?<a class="hash-link" href="#how-can-ttl-be-configured-for-the-redis-keys" title="Direct link to heading">​</a></h4><p>The TTL can be set in 2 ways :-</p><ol><li>Expiring keys after some duration: Set <strong>SINK_REDIS_TTL_TYPE</strong> to <code>DURATION</code> and <strong>SINK_REDIS_TTL_VALUE</strong> to the
duration in seconds.</li><li>Expiring keys at some fixed time: Set <strong>SINK_REDIS_TTL_TYPE</strong> to <code>EXACT_TIME</code> and <strong>SINK_REDIS_TTL_VALUE</strong> to
the <code>UNIX</code> timestamp when the key needs to expire.</li></ol><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="does-it-support-deleting-the-keys">Does it support deleting the keys?<a class="hash-link" href="#does-it-support-deleting-the-keys" title="Direct link to heading">​</a></h4><p>Keys can be deleted by configuring a TTL. To know how to set the TTL, please
look <a href="#how-can-ttl-be-configured-for-the-redis-keys">here</a>.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-are-some-of-the-use-cases-of-this-sink">What are some of the use cases of this sink?<a class="hash-link" href="#what-are-some-of-the-use-cases-of-this-sink" title="Direct link to heading">​</a></h4><p>Some possible use cases could be:-</p><ol><li>High throughput data generated by one application needs to be streamed back to another application for fast lookup
purposes. For example, recommendations based on customer interactions.</li><li>Constant schema but fast changing value of an external entity as published by one application needs to be quickly
communicated to multiple services to maintain an accurate representation of the entity in internal systems. For
example, caching of GCM keys in push notification systems.</li></ol><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-happens-if-the-redis-goes-down">What happens if the Redis goes down?<a class="hash-link" href="#what-happens-if-the-redis-goes-down" title="Direct link to heading">​</a></h4><p>If messages failed to get pushed to Redis because of some error( either due to connection timeout or Redis server
failure), Firehose will retry sending the messages for a fixed number of attempts depending on whether a retryable
exception was thrown. If the messages still failed to get published after retries, Firehose will push these messages to
the DLQ topic if DLQ is enabled via configs. If not enabled, it will drop the messages.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="jdbc-sink">JDBC Sink<a class="hash-link" href="#jdbc-sink" title="Direct link to heading">​</a></h3><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-are-some-of-the-use-cases-of-this-sink-1">What are some of the use cases of this sink?<a class="hash-link" href="#what-are-some-of-the-use-cases-of-this-sink-1" title="Direct link to heading">​</a></h4><p>This sink can be used for cases when one would want to run customised and frequently changing queries on the streaming
data, for joining with other data to do complex analytics or for doing transactional processing.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-monitoring-metrics-are-available-for-this-sink">What monitoring metrics are available for this sink?<a class="hash-link" href="#what-monitoring-metrics-are-available-for-this-sink" title="Direct link to heading">​</a></h4><p>Along with the generic Firehose metrics, this sink also exposes metrics to monitor the following:-</p><ol><li>Connection pool size for the JDBC connection</li><li>Total active and idle connections in the pool</li></ol><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="do-we-need-to-create-tableschema-before-pushing-data-via-jdbc-sink-">Do we need to create table/schema before pushing data via JDBC sink ?<a class="hash-link" href="#do-we-need-to-create-tableschema-before-pushing-data-via-jdbc-sink-" title="Direct link to heading">​</a></h4><p>Yes. The table needs to created before Firehose can stream data to it. The table name can be specified via the config <strong>
SINK_JDBC_TABLE_NAME</strong>.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="any-restriction-of-version-of-supported-database">Any restriction of version of supported database?<a class="hash-link" href="#any-restriction-of-version-of-supported-database" title="Direct link to heading">​</a></h4><p>Firehose JDBC sink works with PostgresSQL ver. 9 and above.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-messages-get-mapped-to-the-queries">How messages get mapped to the queries?<a class="hash-link" href="#how-messages-get-mapped-to-the-queries" title="Direct link to heading">​</a></h4><p>Firehose maps an incoming Kafka message to a query based on 2 configs: <strong>INPUT_SCHEMA_PROTO_TO_COLUMN_MAPPING</strong> and <strong>
SINK_JDBC_UNIQUE_KEYS</strong>. For example, consider a proto as follows:-</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">message Driver {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    int32 driver_id = 1;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    PersonName driver_name = 2;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">message PersonName {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    string fname = 1;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    string lname = 2;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Now consider that the Driver table in the database has 3 columns: <code>id</code>, <code>first_name</code> and <code>last_name</code>. Accordingly, one
can set the <strong>INPUT_SCHEMA_PROTO_TO_COLUMN_MAPPING</strong> config as follows:-</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">{&quot;&quot;1&quot;&quot;:&quot;&quot;id&quot;&quot;, &quot;&quot;2&quot;&quot;:&quot;&quot;{\&quot;&quot;1\&quot;&quot;:\&quot;&quot;first_name\&quot;&quot;, \&quot;&quot;2\&quot;&quot;:\&quot;&quot;last_name\&quot;&quot;}&quot;&quot;}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>This will output a row in the Driver table as follows:-</p><table><thead><tr><th>id</th><th>first_name</th><th>last_name</th></tr></thead><tbody><tr><td>1234</td><td>Chuck</td><td>Norris</td></tr></tbody></table><p>Unique keys as per the table schema can be configured by setting the config <strong>SINK_JDBC_UNIQUE_KEYS</strong>. For
example: <code>order_number,driver_id</code>. When trying to push a message from Kafka to the database table, Firehose will
construct a query which first tries to do an INSERT. If there is a conflict, such as a unique key constraint violation,
the query tries to do an update of all the non-unique columns of the row with appropriate values from the message. If
still there is a conflict, the query won&#x27;t do anything.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-data-types-are-handled">How data types are handled?<a class="hash-link" href="#how-data-types-are-handled" title="Direct link to heading">​</a></h4><p>Firehose can insert/update rows into the DB table with columns of data type string only. For a field of type as a nested
Protobuf message, Firehose will serialise this message into a JSON string. While serialising, it will trim insignificant
whitespaces, retain the default field names as specified in the message descriptor and use the default values for the
field if no value is set. For a field of type collection of messages, the individual messages are converted into JSON
strings and then serialised into a JSON array. The array itself is then serialised into a string. For a field of type
Timestamp, Firehose will parse it into an Instant type and then serialise it as a string. For fields of all other data
types in the Kafka message, the field values are simply serialised to string.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-are-the-database-connections-are-formed">How are the database connections are formed?<a class="hash-link" href="#how-are-the-database-connections-are-formed" title="Direct link to heading">​</a></h4><p>DB connections are created using a Hikari Connection Pool.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-does-jdbc-sink-handles-connection-pooling">How does JDBC sink handles connection pooling?<a class="hash-link" href="#how-does-jdbc-sink-handles-connection-pooling" title="Direct link to heading">​</a></h4><p>JDBC Sink creates a Hikari ConnectionPool (based on max pool size). The connections are used and released back to the
pool while in use. When firehose shuts down, it closes the pool.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="when-and-how-do-the-db-connections-gets-closed">When and how do the DB connections gets closed?<a class="hash-link" href="#when-and-how-do-the-db-connections-gets-closed" title="Direct link to heading">​</a></h4><p>When the main thread is interrupted, Firehose consumer is closed which in turn closes the JDBC sink which in turn closes
the pool.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-to-support-a-new-data-base-which-supports-jdbc-eg-mysql-">How to support a new data base which supports JDBC, e:g MySQL ?<a class="hash-link" href="#how-to-support-a-new-data-base-which-supports-jdbc-eg-mysql-" title="Direct link to heading">​</a></h4><p>Adding of new DB sinks can be supported by making the necessary code changes, such as including the necessary JDBC
driver dependencies, changing the <code>JDBCSinkFactory.java</code> class to create instance of the new sink, extending and
overriding the <code>AbstractSink.java</code> class lifecycle methods to handle the processing of messages, etc.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="any-transaction-locking-provision">Any transaction, locking provision?<a class="hash-link" href="#any-transaction-locking-provision" title="Direct link to heading">​</a></h4><p>As the inserts are independent of each other and depends on events, there is no locking/transaction provision.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="are-there-any-chances-of-race-conditions">Are there any chances of race conditions?<a class="hash-link" href="#are-there-any-chances-of-race-conditions" title="Direct link to heading">​</a></h4><p>In cases where the unique key in the db is different from Kafka key, there can be cases where data from one partition
can override the data from other partition.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="http-sink">HTTP Sink<a class="hash-link" href="#http-sink" title="Direct link to heading">​</a></h3><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-does-the-payload-look-like">How does the payload look like?<a class="hash-link" href="#how-does-the-payload-look-like" title="Direct link to heading">​</a></h4><p>The payload format differs based on the configs <strong>SINK_HTTP_DATA_FORMAT</strong>, <strong>SINK_HTTP_JSON_BODY_TEMPLATE</strong>,
<strong>SINK_HTTP_PARAMETER_SOURCE</strong>, <strong>SINK_HTTP_SERVICE_URL</strong> and <strong>SINK_HTTP_PARAMETER_PLACEMENT</strong>. For details on what these
configs mean, please have a look at the config section.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="does-it-support-delete-calls">Does it support DELETE calls?<a class="hash-link" href="#does-it-support-delete-calls" title="Direct link to heading">​</a></h4><p>At the moment, the HTTP Sink supports only PUT and POST methods.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-many-messages-are-pushed-in-one-call">How many messages are pushed in one call?<a class="hash-link" href="#how-many-messages-are-pushed-in-one-call" title="Direct link to heading">​</a></h4><p>Firehose can support batching of messages in one HTTP call based on special configurations. In such cases, the total
number of messages which are pulled from Kafka in a single fetch can be serialised into one request. This is controlled
by the config <strong>SOURCE_KAFKA_CONSUMER_CONFIG_MAX_POLL_RECORDS</strong>.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-can-i-configure-the-number-of-connections">How can I configure the number of connections?<a class="hash-link" href="#how-can-i-configure-the-number-of-connections" title="Direct link to heading">​</a></h4><p>The max number of HTTP connections can be configured by specifying the config <strong>SINK_HTTP_MAX_CONNECTIONS</strong>.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-data-types-of-request-body-are-supported">What data types of request body are supported?<a class="hash-link" href="#what-data-types-of-request-body-are-supported" title="Direct link to heading">​</a></h4><p>The request body can be sent as a JSON string in which the message proto is parsed into their respective fields in the
request body. It can also be sent in a standard JSON format as below, with each of the fields being set to the
respective encoded serialised string from the Kafka message:</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">    {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">     &quot;topic&quot;:&quot;sample-topic&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">     &quot;log_key&quot;:&quot;CgYIyOm+xgUSBgiE6r7GBRgNIICAgIDA9/y0LigCMAM=&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">     &quot;log_message&quot;:&quot;CgYIyOm+xgUSBgiE6r7GBRgNIICAgIDA9/y0LigCMAM=&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  }</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="does-it-support-client-side-load-balancing">Does it support client side load balancing?<a class="hash-link" href="#does-it-support-client-side-load-balancing" title="Direct link to heading">​</a></h4><p>Yes. The IP/hostname of the Load Balancer fronting the HTTP Sink Servers can be provided to Firehose via
<strong>SINK_HTTP_SERVICE_URL</strong>. Firehose will call this endpoint when pushing messages and then the LB can take care of
distributing the load among multiple sink servers.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="which-authentication-methods-are-supported">Which authentication methods are supported?<a class="hash-link" href="#which-authentication-methods-are-supported" title="Direct link to heading">​</a></h4><p>Firehose supports OAuth authentication for the HTTP Sink.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-can-i-pass-a-particular-input-field-as-a-header-in-request">How can I pass a particular input field as a header in request?<a class="hash-link" href="#how-can-i-pass-a-particular-input-field-as-a-header-in-request" title="Direct link to heading">​</a></h4><ol><li>set <strong>SINK_HTTP_PARAMETER_SOURCE</strong> to either <code>key</code> or <code>message</code>
(based on whether the field one requires in the http request is part of the key or the message in the Kafka record)</li><li>set <strong>SINK_HTTP_PARAMETER_PLACEMENT</strong> to <code>header</code>, and</li><li>set <strong>SINK_HTTP_PARAMETER_SCHEMA_PROTO_CLASS</strong> to the proto class for the input Kafka message and set
<strong>INPUT_SCHEMA_PROTO_TO_COLUMN_MAPPING</strong> to a json value indicating the proto field number from the input message to be
mapped to the header name.</li></ol><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-can-i-pass-a-particular-input-field-as-a-query-param-in-request">How can I pass a particular input field as a query param in request?<a class="hash-link" href="#how-can-i-pass-a-particular-input-field-as-a-query-param-in-request" title="Direct link to heading">​</a></h4><ol><li>set <strong>SINK_HTTP_PARAMETER_SOURCE</strong> to either <code>key</code> or <code>message</code>
(based on whether the field one requires in the http request is part of the key or the message in the Kafka record),</li><li>set <strong>SINK_HTTP_PARAMETER_PLACEMENT</strong> to <code>query</code>, and</li><li>set <strong>SINK_HTTP_PARAMETER_SCHEMA_PROTO_CLASS</strong> to the proto class for the input Kafka message and set
<strong>INPUT_SCHEMA_PROTO_TO_COLUMN_MAPPING</strong> to a json value indicating the proto field number from the input message to be
mapped to the query parameter name.</li></ol><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-happens-if-my-services-fails-">What happens if my services fails ?<a class="hash-link" href="#what-happens-if-my-services-fails-" title="Direct link to heading">​</a></h4><p>If messages failed to get pushed to the HTTP endpoint with a retryable status code as specified via
<strong>SINK_HTTP_RETRY_STATUS_CODE_RANGES</strong> config, Firehose will retry sending the messages for a fixed number of attempts.
If the messages still failed to get published after retries, Firehose will push these messages to the DLQ topic if DLQ is
enabled via configs. If not enabled, it will drop the messages.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-are-http-connections-handled-long-lived">How are HTTP connections handled, long lived?<a class="hash-link" href="#how-are-http-connections-handled-long-lived" title="Direct link to heading">​</a></h4><p>A connection pool is created with max number of connections set to <strong>SINK_HTTP_MAX_CONNECTIONS</strong> configuration. Firehose
doesn&#x27;t support maintaining keep-alive connections and hence connections are renewed every-time the default TTL for
PoolingHttpClientConnectionManager expires.For more details,
see <a href="https://hc.apache.org/httpcomponents-client-4.5.x/current/httpclient/apidocs/org/apache/http/impl/conn/PoolingHttpClientConnectionManager.html" target="_blank" rel="noopener noreferrer">here</a></p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-is-logrequest-config">What is logRequest config?<a class="hash-link" href="#what-is-logrequest-config" title="Direct link to heading">​</a></h4><p>If the response received from the http sink endpoint is null or if the status code of the response is within the range
of <strong>SINK_HTTP_REQUEST_LOG_STATUS_CODE_RANGES</strong> config, then the request payload is logged. e;g, <code>200-500</code></p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-is-shouldretry-config">What is shouldRetry config?<a class="hash-link" href="#what-is-shouldretry-config" title="Direct link to heading">​</a></h4><p>If the response received from the http sink endpoint is null or if the status code of the response is within the range
of <strong>SINK_HTTP_RETRY_STATUS_CODE_RANGES</strong> config, then the request is again retried for a configured number of attempts
with backoff. e;g, <code>400-500</code></p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-happens-in-case-of-null-response">What happens in case of null response?<a class="hash-link" href="#what-happens-in-case-of-null-response" title="Direct link to heading">​</a></h4><p>If the request to the sink fails with a null status code, Firehose will attempt to retry calling the endpoint for a
configured number of attempts with backoff. After that, if DLQ is enabled, the messages are pushed to DLQ queue with
backoff. If not enabled, no more attempts are made.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-happens-in-case-of-null-status-code-in-a-non-null-response">What happens in case of null status code in a non-null response?<a class="hash-link" href="#what-happens-in-case-of-null-status-code-in-a-non-null-response" title="Direct link to heading">​</a></h4><p>If the request to the sink fails with a null status code, Firehose will drop the messages and not retry sending them
again.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="when-is-a-message-dropped">When is a message dropped?<a class="hash-link" href="#when-is-a-message-dropped" title="Direct link to heading">​</a></h4><p>If the request to the sink fails with a retryable status code as specified via <strong>SINK_HTTP_RETRY_STATUS_CODE_RANGES</strong>
config, Firehose will attempt to retry calling the endpoint for a configured number of attempts with backoff. After
that, if DLQ is enabled, the messages are pushed to DLQ queue with backoff. If DLQ is disabled, messages are dropped.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-is-the-difference-between-parameterised-vs-dynamic-url-">What is the difference between Parameterised vs Dynamic Url ?<a class="hash-link" href="#what-is-the-difference-between-parameterised-vs-dynamic-url-" title="Direct link to heading">​</a></h4><p>In parameterised query and parameterised header HTTP sink mode, the incoming Kafka message is converted to query
parameters and header parameters in the HTTP request respectively. In case of dynamic url HTTP sink mode, the input
Kafka message is parsed into the request body.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="for-parameterised-header-how-is-the-data-added">For parameterised header, how is the data added?<a class="hash-link" href="#for-parameterised-header-how-is-the-data-added" title="Direct link to heading">​</a></h4><ol><li>set <strong>SINK_HTTP_PARAMETER_SOURCE</strong> to either <code>key</code> or <code>message</code> (based on whether the field one requires in the http
request is part of the key or the message in the Kafka record),</li><li>set <strong>SINK_HTTP_PARAMETER_PLACEMENT</strong> to <code>header</code>, and</li><li>set <strong>SINK_HTTP_PARAMETER_SCHEMA_PROTO_CLASS</strong> to the Protobuf class for the input Kafka message and
<strong>INPUT_SCHEMA_PROTO_TO_COLUMN_MAPPING</strong> to a json value indicating the proto field number from the input message to be
mapped to the header name.</li></ol><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="general-queries">General Queries<a class="hash-link" href="#general-queries" title="Direct link to heading">​</a></h3><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="which-java-versions-firehose-work-with">Which Java versions firehose work with?<a class="hash-link" href="#which-java-versions-firehose-work-with" title="Direct link to heading">​</a></h4><p>Firehose has been developed and tested to work with Java 8. Compatibility with lower or higher versions is not known and
hence, needs to be tested prior to use.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-does-the-execution-work-is-the-entire-process-sync-or-async-">How does the execution work? Is the entire process sync or async ?<a class="hash-link" href="#how-does-the-execution-work-is-the-entire-process-sync-or-async-" title="Direct link to heading">​</a></h4><p>The execution works as follows:</p><ul><li>Get messages from Kafka</li><li>Filter the messages (optional)</li><li>Push these messages to sink: All the existing sink types follow the same contract/lifecycle defined
in <code>AbstractSink.java</code>. It consists of two stages:<ul><li><strong>Prepare</strong>: Transformation over-filtered messages’ list to prepare the sink-specific insert/update client
requests.</li><li><strong>Execute</strong>: Requests created in the prepare stage are executed at this step and a list of failed
messages is returned (if any) for retry.</li></ul></li><li>If the push to the sink fails with a retryable exception, Firehose will attempt to retry pushing the messages for a
configured number of attempts with backoff. After that, if DLQ is enabled, the messages are pushed to DLQ queue with
backoff. If DLQ is disabled, messages are dropped.</li><li>Captures telemetry and success/failure events and send them to Telegraf</li><li>Repeat the process above again</li></ul><p>Firehose can configure its Kafka consumer to work in either sync or async mode. For more details, you can look
<a href="/firehose/concepts/consumer">here</a></p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="is-there-any-code-snippet-which-shows-how-i-can-produce-sample-message-in-supported-data-format">Is there any code snippet which shows how I can produce sample message in supported data format?<a class="hash-link" href="#is-there-any-code-snippet-which-shows-how-i-can-produce-sample-message-in-supported-data-format" title="Direct link to heading">​</a></h4><p>Following is an example to demonstrate how to create a Protobuf message and then produce it to a Kafka cluster. Firstly,
create a  <code>.proto</code>  file containing all the required field names and their corresponding integer tags. Save it in a new
file named  <code>person.proto</code></p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">syntax = &quot;proto2&quot;;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">package tutorial;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">option java_multiple_files = true;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">option java_package = &quot;com.example.tutorial.protos&quot;;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">option java_outer_classname = &quot;PersonProtos&quot;;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">message Person {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  optional string name = 1;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  optional int32 id = 2;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  optional string email = 3;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  enum PhoneType {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    MOBILE = 0;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    HOME = 1;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    WORK = 2;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  message PhoneNumber {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    optional string number = 1;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    optional PhoneType type = 2 [default = HOME];</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  repeated PhoneNumber phones = 4;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Next, compile your  <code>.proto</code>  file using Protobuf compiler i.e.  <code>protoc</code>.This will generate Person, PersonOrBuilder and
PersonProtos Java source files. Specify the source directory (where your application&#x27;s source code lives – the current
directory is used if you don&#x27;t provide a value), the destination directory (where you want the generated code to go;
often the same as  <code>$SRC_DIR</code>), and the path to your  <code>.proto</code></p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">protoc -I=$SRC_DIR --java_out=$DST_DIR $SRC_DIR/person.proto</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Lastly, add the following lines in your Java code to generate a POJO (Plain Old Java Object) of the Person proto class
and serialize it to a byte array, using the  <code>toByteArray()</code>  method of
the  <a href="https://www.javadoc.io/static/com.google.protobuf/protobuf-java/3.5.1/com/google/protobuf/GeneratedMessageV3.html" target="_blank" rel="noopener noreferrer">com.google.protobuf.GeneratedMessageV3</a>
class. The byte array is then sent to the Kafka cluster by the producer.</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">KafkaProducer&lt;byte[], byte[]&gt; producer = new KafkaProducer&lt;&gt;(properties);</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Person john = Person.newBuilder()</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                .setId(87182872)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                .setName(&quot;John Doe&quot;)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                .setEmail(&quot;jdoe@example.com&quot;)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                .addPhones(</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                        Person.PhoneNumber.newBuilder()</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                                .setNumber(&quot;555-4321&quot;)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                                .setType(Person.PhoneType.HOME))</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                .build();</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">producer.send(new ProducerRecord&lt;byte[], byte[]&gt;(topicName, john.toByteArray()));</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Refer  <a href="https://developers.google.com/protocol-buffers" target="_blank" rel="noopener noreferrer">https://developers.google.com/protocol-buffers</a>  for more info
on how to create protobuf files.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="why-protobuf--can-it-support-other-formats-like-json-avro-thrift-">Why Protobuf ? Can it support other formats like JSON, AVRO, Thrift ?<a class="hash-link" href="#why-protobuf--can-it-support-other-formats-like-json-avro-thrift-" title="Direct link to heading">​</a></h4><p>Protocol buffers are Google&#x27;s language-neutral, platform-neutral, extensible mechanism for serialising structured data.
Data streams on Kafka topics are bound to a Protobuf schema. Protobuf is much more lightweight that other schema formats
like JSON, since it encodes the keys in the message to integers.</p><p>ElasticSearch and MongoDB sink support both JSON and Protobuf as the input schema. For other sinks, Firehose currently
supports only Protobuf. Support for JSON and Avro is planned and incorporated in the future roadmap.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="does-firehose-have-support-for-ssl-on-kafka">Does firehose have support for SSL on Kafka?<a class="hash-link" href="#does-firehose-have-support-for-ssl-on-kafka" title="Direct link to heading">​</a></h4><p>No, the Kafka consumer used in Firehose doesn&#x27;t support SSL.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-does--firehose-create-the-kafka-consumer">How does  Firehose create the Kafka consumer?<a class="hash-link" href="#how-does--firehose-create-the-kafka-consumer" title="Direct link to heading">​</a></h4><p>When Firehose starts, it creates as many Kafka consumers as specified by the config <strong>APPLICATION_THREAD_COUNT</strong> (
default set to 1). Each consumer runs on a separate thread. Please
look <a href="https://github.com/odpf/firehose/blob/main/docs/reference/configuration.md#kafka-consumer" target="_blank" rel="noopener noreferrer">here</a> for more details
on how to configure the Kafka consumer.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="can-i-change-the-consumer-group-name-or-reset-it--overall-how-do-i-handle-my-consumer-group-operations">Can I change the consumer group name or reset it ? Overall, how do I handle my consumer group operations?<a class="hash-link" href="#can-i-change-the-consumer-group-name-or-reset-it--overall-how-do-i-handle-my-consumer-group-operations" title="Direct link to heading">​</a></h4><p>Yes, the Kafka consumer group name can be changed by specifying the following configuration <strong>SOURCE_KAFKA_CONSUMER_GROUP_ID</strong>. The Kafka consumer in Firehose can also be reset. However, the default behaviour is
currently set to read from the latest offset. To know further details on how to tune the Kafka consumer group, you can
have a look at the Kafka consumer configurations already exposed by
Firehose <a href="https://github.com/odpf/firehose/blob/main/docs/reference/configuration/kafka-consumer-1.md#kafka-consumer" target="_blank" rel="noopener noreferrer">here</a>.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-are-the-advantages-of-firehose-over-kafka-connect-">What are the advantages of Firehose over Kafka connect ?<a class="hash-link" href="#what-are-the-advantages-of-firehose-over-kafka-connect-" title="Direct link to heading">​</a></h4><ul><li><strong>Ease of use:</strong>  Firehose is easier to install, and using different sinks only requires changing a few
configurations. However, Kafka Connect requires connectors to be installed across all the worker nodes within the
cluster when used in distributed mode.</li><li><strong>Filtering:</strong>  Value-based filtering is much easier to implement as compared to Kafka Connect. Requires no additional
plugins/schema-registry to be installed.</li><li><strong>Extensible:</strong>  Provides a comprehensible abstract sink contract making it easier to add a new sink in Firehose.
Firehose also comes with an inbuilt serialisation/deserialisation and doesn&#x27;t require any converters and serialisers
when implementing a new sink.</li><li><strong>Easy monitoring:</strong>  Firehose provides a detailed health dashboard (Grafana) for effortless monitoring.</li><li><strong>Connectors:</strong>  Some of the Kafka connect available connectors usually have limitations. It&#x27;s usually rare to find
all the required features in a single connector and so is to find documentation for the same.</li><li><strong>Fully open-source:</strong>  Firehose is completely open-source while separation of commercial and open-source features is
not very structured in Kafka Connect and for monitoring and advanced features, confluent control center requires an
enterprise subscription.</li></ul><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-problems-does-firehose-solve">What problems does Firehose solve?<a class="hash-link" href="#what-problems-does-firehose-solve" title="Direct link to heading">​</a></h4><p>Every micro-service needs its own sink to be developed for common operations such as streaming data from Kafka to data
lakes or other endpoints, along with real-time filtering, parsing, and monitoring of the sink.</p><p>With Firehose, you don&#x27;t need to write sink code for every such microservice, or manage resources to sink data from
Kafka server to your database/service endpoint. Having provided all the configuration parameters of the sink, Firehose
will create, manage and monitor one for you. It also automatically scales to match the throughput of your data and
requires no ongoing administration.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="can-i-do-any-transformations-before-sending-data-to-sink-for-example-filtering-">Can I do any transformations before sending data to sink, for example filtering ?<a class="hash-link" href="#can-i-do-any-transformations-before-sending-data-to-sink-for-example-filtering-" title="Direct link to heading">​</a></h4><p>Yes, Firehose provides JEXL based filters based on the fields in key or message of the Kafka record. Read
the <a href="https://github.com/odpf/firehose/blob/main/docs/concepts/filters.md" target="_blank" rel="noopener noreferrer">Filters</a> section for further details.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-to-optimise-parallelism-based-on-input-rate-of-kafka-messages-does-it-depend-on-sink-">How to optimise parallelism based on input rate of Kafka messages? Does it depend on sink ?<a class="hash-link" href="#how-to-optimise-parallelism-based-on-input-rate-of-kafka-messages-does-it-depend-on-sink-" title="Direct link to heading">​</a></h4><p>You can increase the workers in the Firehose which will effectively multiply the number of records being processed by
Firehose. Adding some sort of filter condition in the Firehose to ignore unnecessary messages in the topic would help
you bring down the volume of data being processed by the sink.Firehose can also be configured for its Kafka consumer to
work in <a href="/firehose/concepts/consumer">async mode</a>, thereby allowing it to do offset management and commit asynchronously
improving performance.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-are-the-retry-mechanisms-in-firehose">What are the retry mechanisms in firehose?<a class="hash-link" href="#what-are-the-retry-mechanisms-in-firehose" title="Direct link to heading">​</a></h4><p>If the push to the sink fails with a retryable error, Firehose will attempt to retry pushing the messages for a
configured number of attempts with backoff. If the messages still fail to get pushed, and if DLQ is enabled, the
messages are pushed to DLQ queue with backoff. If push fails and DLQ is disabled, messages are dropped.</p><p>Starting with version 0.2, Firehose also provides the ability to tag different error types under a specific scope: <code>DLQ</code>
, <code>RETRY</code> or <code>FAIL</code>. This enables Firehose to effectively determine at runtime as to what should be the strategy when a
particular error is encountered. For more details, please look at <code>ErrorConfig.java</code> class and classes in
the <code>io.odpf.firehose.error</code> package.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="which-kafka-client-configs-are-available-">Which Kafka client configs are available ?<a class="hash-link" href="#which-kafka-client-configs-are-available-" title="Direct link to heading">​</a></h4><p>Firehose provides various Kafka client configurations.
Refer <a href="https://github.com/odpf/firehose/blob/main/docs/reference/configuration/kafka-consumer-1.md" target="_blank" rel="noopener noreferrer">Kafka Consumer Configurations</a>
section.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-all-data-formats-are-supported">What all data formats are supported?<a class="hash-link" href="#what-all-data-formats-are-supported" title="Direct link to heading">​</a></h4><p>ElasticSearch, Bigquery and MongoDB sink support both JSON and Protobuf as the input schema. For other sinks, we currently support
only Protobuf. Support for JSON and Avro is planned and incorporated in the roadmap.</p><p>Protocol buffers are Google&#x27;s language-neutral, platform-neutral, extensible mechanism for serialising structured data.
When <code>INPUT_SCHEMA_DATA_TYPE=protobuf</code> Data streams on Kafka topics are bound to a Protobuf schema. Follow the instructions
in  <a href="https://developers.google.com/protocol-buffers/docs/javatutorial" target="_blank" rel="noopener noreferrer">this article</a>  on how to create, compile and
serialize a Protobuf object to send it to a binary OutputStream.
Refer  <a href="https://developers.google.com/protocol-buffers/docs/proto3" target="_blank" rel="noopener noreferrer">this guide</a>  for detailed Protobuf syntax and rules
to create a  <code>.proto</code>  file.
When <code>INPUT_SCHEMA_DATA_TYPE=json</code> data streams on kafka topics are bound to having a valid json message.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="can-we-select-particular-fields-from-the-input-message">Can we select particular fields from the input message?<a class="hash-link" href="#can-we-select-particular-fields-from-the-input-message" title="Direct link to heading">​</a></h4><p>Firehose will send all the fields of the incoming messages to the specified sink. But you can configure your sink
destination/ database to consume only the required fields.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-do-i-configure-the-protobuf-schema-for-the-topic-firehose-needs-to-consume">How do I configure the Protobuf schema for the topic Firehose needs to consume?<a class="hash-link" href="#how-do-i-configure-the-protobuf-schema-for-the-topic-firehose-needs-to-consume" title="Direct link to heading">​</a></h4><p>Generated Protobuf Descriptors are hosted behind a Stencil server artifactory/HTTP endpoint. This endpoint URL and the
ProtoDescriptor class that the Firehose deployment should use to deserialise raw data with is configured in Firehose in
the environment variables <strong>SCHEMA_REGISTRY_STENCIL_URLS</strong> and <strong>INPUT_SCHEMA_PROTO_CLASS</strong>  respectively.</p><p>The Proto Descriptor set of the Kafka messages must be uploaded to the Stencil server.
Refer  <a href="https://github.com/odpf/stencil/blob/master/docs/guides/quick_start.md" target="_blank" rel="noopener noreferrer">this guide</a>  on how to setup and
configure the Stencil server.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-is-stencil-in-context-of-firehose-">What is Stencil in context of firehose ?<a class="hash-link" href="#what-is-stencil-in-context-of-firehose-" title="Direct link to heading">​</a></h4><p>ODPF Stencil API is a dynamic schema registry for hosting and managing versions of Protobuf descriptors. The schema
handling i.e., find the mapped schema for the topic, downloading the descriptors, and dynamically being notified
of/updating with the latest schema is abstracted through the Stencil library.</p><p>The Stencil Client is a proprietary library that provides an abstraction layer, for schema handling. Schema caching,
dynamic schema updates are features of the stencil client library.</p><p>Refer  <a href="https://odpf.gitbook.io/stencil/" target="_blank" rel="noopener noreferrer">this article</a>  for further information of the features, configuration and
deployment instructions of the Stencil API. Source code of Stencil Server and Client API can be found in
its  <a href="https://github.com/odpf/stencil" target="_blank" rel="noopener noreferrer">Github repository</a>.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="will-i-have-any-data-loss-if-my-firehose-is-failed-">Will I have any data loss if my firehose is failed ?<a class="hash-link" href="#will-i-have-any-data-loss-if-my-firehose-is-failed-" title="Direct link to heading">​</a></h4><p>Firehose follows an <em>at-least</em> once policy. After a batch of messages is pushed to the sink successfully, Firehose
commits the offset before the consumer polls another batch from Kafka. Thus, failed messages are not committed.</p><p>So, when Firehose is restarted, the Kafka Consumer automatically starts pulling messages from the last committed offset
of the consumer group. No data loss occurs.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-does-firehose-handle-failed-messages">How does firehose handle failed messages?<a class="hash-link" href="#how-does-firehose-handle-failed-messages" title="Direct link to heading">​</a></h4><p>If the push to the sink fails with a retryable error, Firehose will attempt to retry pushing the messages for a
configured number of attempts with backoff. If the messages still fail to get pushed, and if DLQ is enabled, the
messages are pushed to DLQ queue with backoff. If push fails and DLQ is disabled, messages are dropped.</p><p>Starting with version 0.2, Firehose also provides the ability to tag different error types under a specific scope: <code>DLQ</code>
, <code>RETRY</code> or <code>FAIL</code>. This enables Firehose to effectively determine at runtime as to what should be the strategy when a
particular error is encountered. For more details, please look at <code>ErrorConfig.java</code> class and classes in
the <code>io.odpf.firehose.error</code> package.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-all-metrics-are-produced-for-me-to-monitor-">What all metrics are produced for me to monitor ?<a class="hash-link" href="#what-all-metrics-are-produced-for-me-to-monitor-" title="Direct link to heading">​</a></h4><p>Firehose exposes critical metrics to monitor the health of your delivery streams and take any necessary actions. Refer
the <a href="https://github.com/odpf/firehose/blob/main/docs/reference/metrics.md" target="_blank" rel="noopener noreferrer">Metrics</a> section for further details on each
metric.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-kind-of-delivery-guarantees-does-firehose-provide-is-it-different-from-what-kafka-is-tuned-for">What kind of delivery guarantees does Firehose provide? Is it different from what Kafka is tuned for?<a class="hash-link" href="#what-kind-of-delivery-guarantees-does-firehose-provide-is-it-different-from-what-kafka-is-tuned-for" title="Direct link to heading">​</a></h4><p>Firehose provides at-least once delivery guarantee. In case of a failure or restart, the Kafka Consumer in Firehose will
automatically starts pulling messages from the last committed offset of the consumer group. Hence, no data loss occurs.</p><p>If the push to the sink fails with a retryable error, Firehose will attempt to retry pushing the messages for a
configured number of attempts with backoff. If the messages still fail to get pushed, and if DLQ is enabled, the
messages are pushed to DLQ queue with backoff. If push fails and DLQ is disabled, messages are dropped.</p><p>Starting with version 0.2, Firehose also provides the ability to tag different error types under a specific scope: <code>DLQ</code>
, <code>RETRY</code> or <code>FAIL</code>. This enables Firehose to effectively determine at runtime as to what should be the strategy when a
particular error is encountered. For more details, please look at <code>ErrorConfig.java</code> class and classes in
the <code>io.odpf.firehose.error</code> package.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-happens-if-my-firehose-is-stopped-and-kafka-retention-is-for-few-days">What happens if my firehose is stopped and Kafka retention is for few days?<a class="hash-link" href="#what-happens-if-my-firehose-is-stopped-and-kafka-retention-is-for-few-days" title="Direct link to heading">​</a></h4><p>When firehose restarts, it will start reading from the last committed offset. If the message at the last committed
offset for the partition was deleted due to retention period, the following 3 cases can happen:-</p><ol><li>If the config <strong>SOURCE_KAFKA_CONSUMER_CONFIG_AUTO_OFFSET_RESET</strong> is set to <code>latest</code>, then Firehose will start
consuming from the latest offset for the partition.</li><li>If the config <strong>SOURCE_KAFKA_CONSUMER_CONFIG_AUTO_OFFSET_RESET</strong> is set to <code>earliest</code>, then Firehose will start
consuming from the earliest available offset for the partition. For example, if the committed offset was 10, but the
earliest available offset is now 21, Firehose will start reading from offset 21 for the partition. This is also the
default behaviour in case this config is not specified at all.</li><li>If the config <strong>SOURCE_KAFKA_CONSUMER_CONFIG_AUTO_OFFSET_RESET</strong> is set to <code>none</code> or anything else, then Firehose
will terminate with an abnormal status code.</li></ol></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/odpf/firehose/edit/master/docs/docs/reference/faq.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_eYIM" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vbeJ"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/firehose/reference/core-faqs"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">FAQs</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/firehose/reference/glossary"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Glossary</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#firehose-sinks" class="table-of-contents__link toc-highlight">Firehose Sinks</a><ul><li><a href="#blob-sink" class="table-of-contents__link toc-highlight">Blob Sink</a></li><li><a href="#bigquery-sink" class="table-of-contents__link toc-highlight">BigQuery Sink</a></li><li><a href="#redis-sink" class="table-of-contents__link toc-highlight">Redis Sink</a></li><li><a href="#jdbc-sink" class="table-of-contents__link toc-highlight">JDBC Sink</a></li><li><a href="#http-sink" class="table-of-contents__link toc-highlight">HTTP Sink</a></li><li><a href="#general-queries" class="table-of-contents__link toc-highlight">General Queries</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Open DataOps Foundation © 2022</div></div></div></footer></div>
<script src="/firehose/assets/js/runtime~main.daa3e954.js"></script>
<script src="/firehose/assets/js/main.2586b491.js"></script>
</body>
</html>